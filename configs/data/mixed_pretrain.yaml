# Mixed pretraining data configuration
# All sources are commercially friendly

name: mixed_pretrain

# Dataset sources with weights (all commercially friendly)
sources:
  - name: dclm
    path: mlfoundations/dclm-baseline-1.0-parquet
    weight: 0.25
    license: CC-BY-4.0

  - name: fineweb_edu
    path: HuggingFaceFW/fineweb-edu
    subset: sample-10BT
    weight: 0.30
    license: ODC-By

  - name: github_code
    path: nick007x/github-code-2025
    weight: 0.15
    license: MIT

  - name: finemath
    path: HuggingFaceTB/finemath
    subset: finemath-3plus
    weight: 0.15
    license: ODC-By

  - name: slimpajama
    path: cerebras/SlimPajama-627B
    weight: 0.15
    license: Apache-2.0

# Data processing settings
streaming: true
shuffle: true
shuffle_buffer_size: 10000

# Tokenization
max_seq_length: 512
packing: true

# Cache
cache_dir: null  # Use default cache
